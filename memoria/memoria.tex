\documentclass{report}

\setlength{\textwidth}{150mm}
%\setlength{\textheight}{195mm}
\setlength{\oddsidemargin}{9mm}
%\setlength{\evensidemargin}{28mm}
%\setlength{\topmargin}{-10mm}


\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\definecolor{light-gray}{gray}{0.96}
\graphicspath{ {images/} }
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}

\begin{document}
\begin{titlepage}
\centering

\begin{figure}[t]
\includegraphics[scale=0.5]{images/urjc_logo.png}
\centering
\vspace{0.5cm} %Espacios despúes de la imagen
\end{figure}

{\scshape\Large Escuela Técnica Superior de Ingeniería de Telecomunicación \par}
\vspace{1cm}
{\scshape\Large Grado en Ingeniería en Sistemas de Telecomunicación \par}
\vspace{3cm}
{\bfseries\LARGE Procesamiento de imágenes y Deep Learning para un robot educativo simulado
en navegador web\par}
\vspace{3cm}
{\itshape\Large Trabajo fin de grado \par}
\vfill
{\Large Autor: }
{\Large Jorge Cruz de la Haza \par}
{\Large Tutor: }
{\Large Dr. Jose María Cañas Plaza \par}
{\Large Co-Tutor: }
{\Large Prf. Julio Manuel Vega Pérez \par}
\vfill
{\Large Curso Académico 2019/2020 \par}
\end{titlepage} 

\renewcommand{\abstractname}{\Large Resumen}
\begin{abstract}
Hablar de Robótica, Kibotics y la ciudad simulada.


\end{abstract}

\setcounter{tocdepth}{3} %Para que aparezcan las subsubsection{}
\renewcommand{\contentsname}{Índice general}
\tableofcontents
\clearpage

\renewcommand{\listfigurename}{Índice de figuras}
\listoffigures

\renewcommand{\listtablename}{Índice de tablas}
\listoftables

\renewcommand{\chaptername}{Capítulo}
\chapter{Introducción}

En este capítulo se expone el contexto general que envuelve este proyecto, así como situarlo dentro del mismo. Se explica qué es la robótica, el Deep Learning (o aprendizaje profundo) en visión artificial y, al mismo tiempo, la importancia que va cobrando cada vez más la robótica en la educación. Finalmente se presentarán algunas plataformas robóticas educativas interesantes, así como la plataforma utilizada para este proyecto. En concreto esta plataforme se 

\section{Robótica}

Isaac Asimov fue un escritor ruso, cuya obra principal se basó en la ciencia ficción, la historia y la divulgación científica. Debido a la gran cantidad de publicaciones relacionadas con los robots (de ficción y no ficción) es considerado como \textit{El Padre de la robótica}. En 1950 publicó \textit{''Yo, Robot''}, una colección de relatos que tratan sobre robots inteligentes, y en los que Isaac predijo el aumento de la industria de la robótica. El origen de la palabra robot proviene de la palabra eslava \textit{robota}, que hace referencia al trabajo que se realiza de manera forzada. 
\\

Hoy en día la robótica hace referencia a la disciplina cietífica que aglutina distintas ramas tecnológicas (como la mecánica, la física, las matemáticas, o la inteligencia artificial), con el propósito de diseñar máquinas robotizadas (robots) que sean capaces de realizar tareas de manera autónoma. El objetivo del diseño de estas máquinas es poder facilitar la vida de las personas y, en algunos casos, llegar a sustiturlas en algunos trabajos. Las aplicaciones de la robótica pueden llegar a ser realmente útiles en las tareas cotidianas y son de especial interés  cuando los robots pueden simular el comportamiento humano en tareas que puedan suponer un riesgo para las personas, evitando así que estas corran ese riesgo.
\\

Existen diferentes tipos de clasificaciones de robots entre las que podemos encontrar:

\begin{enumerate}
	\item En función de su utilidad:
	\begin{itemize}
		\item Industriales. Son aquellos que están destinados a realizar determinados procesos o tareas de una forma automática, principalmente orientados a procesos de fabriación o manufacturación. Se caracterizan por tener brazos mecánicos o poliarticulados con distintos ejes, que pueden ser móviles o fijos. Estos tipos de robots no necesitan la manipulación de una persona para realizar su tarea.
		
		\item Móviles. Disponen de mecanismos que les permiten desplazarse, siguiendo un camino teleoperado o identificando los elementos de su entorno y desplazándose de manera autónoma. Son útiles para el transporte, cadenas de fabricación, envíos, etc. Se caracterizan por tener un relativo elevado nivel de inteligencia.
		
		\item Médicos. Destinados a realizar distintas funciones aplicadas al ámbito de la medicina, tales como robots que permitan escanear información del cerebro, robots con medidas de precisión que indiquen el punto exacto donde hay que realizar una incisión para una operación, telediagnóstico o telecirugía. Recientemente está en auge la industria robótica dedicada a fabricar prótesis dotadas de sistemas de mando que permiten simular los movimientos y funciones de los órganos.
		
		\item Androides. Son quellos fabricados con apariencia y características similares a los humanos. Imitan acciones o conductas de las personas de manera autónoma. A día de hoy este tipo de robots no está muy evolucionado debido a que resulta complicado emular y coordinar los movimientos y comportamientos humanos. Dentro de este grupo se incluyen los robots zoomórficos, que se caracterizan por imitar los movimientos y características de distintos seres vivos.
		 
		\item Híbridos. El resto de robots que no pueden clasificarse en ninguna de las categorías anteriores o que presentan características de dos o más categorías al mismo tiempo.

\renewcommand{\figurename}{Figura}		
\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.25\textwidth}
    \includegraphics[width=\textwidth, height=\textwidth]{images/robot_industrial.png}
    \caption{Robot industrial}
    \label{fig:f1}
  \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
    \includegraphics[width=\textwidth, height=\textwidth]{images/robot_movil.png}
    \caption{Robot móvil}
    \label{fig:f2}
  	\end{subfigure}
  	\begin{subfigure}[b]{0.25\textwidth}
    \includegraphics[width=1.3\textwidth, height=\textwidth]{images/robot_medico.png}
    \caption{Robot médico}
    \label{fig:f3}
  	\end{subfigure}
  	\begin{subfigure}[b]{0.25\textwidth}
    \includegraphics[width=0.7\textwidth, height=\textwidth]{images/robot_androide.png}
    \caption{Robot androide}
    \label{fig:f3}
  	\end{subfigure}
  	\caption{Tipos de robots en función de su utilidad.}
\end{figure}
	\end{itemize}


	\item En función de su operación:
	\begin{itemize}
		\item Robots teleoperados. Aquellos robots cuyas acciones que son controladas de manera remota por una persona. Este tipo de robots se utilizan principalmente en tareas que puedan suponer un riesgo de salud para las personas, pero que al mismo tiempo requieran un tratamiento humano para su elaboración (por ejemplo, robots para desactivación de bombas).
		
		\item Robots autónomos. Por el contrario, este tipo de robots son capaces de realizar taeras de manera independiente ya que son máquinas mucho más complejas y son dotadas con cierta inteligencia. Este tipo de robots son utilizados en algunos escenarios en los que el robot es capaz de analizar el entorno que le rodea para generar una respuesta por sí mismo. Este tipo de robots es el que utilizaremos para desarrollar este proyecto.
	\end{itemize}
\end{enumerate}

A día de hoy existen numerosas aplicaciones de la robótica en nuestra vida cotidiana, y cada vez son más los casos de uso debido a su gran utilidad y a la mejora que producen en la vida de las personas. El campo de la robótica está en continuo investigación y desarrollo, debido en gran parte a la gran velocidad con la que ha avanzado la tecnología en los últimos años. Algunos de los ejemplos más relevantes del uso de la robótica en nuestra vida cotidiana son; el coche autónomo de Tesla, dotado de cámaras y sensores que permiten que el coche sea capaz de circular de manera autónoma e identificar su entorno para actuar en consecuencia (detenerse en caso de que haya un peatón justo en frente, corregir la trayectoria en caso de pisar una línea continua…);  aspiradoras Roomba, capaces de identificar y memorizar la superficie aspirada para recorrer  y limpiar toda la vivienda de manera autónoma;  el robot DaVinci, un instrumento quirúrgico capaz de reproducir los movimientos de un cirujano sin latencia y con precisión, que permite reducir los temblores del cirujano y hace que pueda operar sentado de una manera más cómoda; centros logísticos de Amazon equipados con robots que ayudan a los humanos a seleccionar, clasificar, transportar y almacenar paquetes.
\\

Sin duda la robótica es un campo muy útil, en continuo desarrollo y de gran interés común, por lo que realizar un trabajo de fin de grado en este ámbito resulta interesante ya que es un tema que está a la orden del día y son muchos los casos de uso realizables en la vida real.


\section{Deep Learning en Visión Artifical}
Muchos de los robots actuales tienen una cámara como sensor principal que les permite tener cierto conocimiento del entorno en el que se encuentran, por lo que pueden actuar e interaccionar con los elementos que les rodean en consecuencia. Por tanto, estos robots se convierten en sistemas inteligentes cuya inteligencia está dotada principalmente por dos elementos: la cámara (visión artificial) y los algoritmos que procesan esas imágenes (Deep Learning).
\\

El \textit{Deep Learning} (aprendizaje profundo) es una rama de la ingeniería que permite a un ordenador crear conceptos y lógicas complejas a partir de otras más simples, todo ello mediante un aprendizaje automático. Los algoritmos de DeepLearning permiten crear un conocimiento en una máquina mediante un entrenamiento e intentan modelar abstraciones de alto nivel (por ejemplo, percepciones humanas tales como identificar distintos objetos en una imagen) utilizando arquitecturas compuestas de transformaciones no lineales múltiples. Una de las grandes ventajas del Deep Learning es que, a medida que aumenta la cantidad de datos y de ejemplos utilizados, mejora la precisión del algoritmo.
\\

En un contexto más amplio, el \textit{Deep Learning} es una familia de técnicas dentro del Aprendizaje Automático, que es parte de la Inteligencia Artificial (IA). La IA que es la ciencia que estudia la inteligencia que es capaz de desarrollar una máquina mediante la programación. Dentro de la Inteligencia Artificial podemos encontrar el Aprendizaje Máquina (\textit{Machine Learning}), definido como la capacidad de las máquinas para aprender, construyendo modelos o patrones a partir de datos no procesados); el Aprendizaje de la Representacion (\textit{Representation Learning}), que emplea el Aprendizaje Máquina para descubrir automáticamente representaciones o clasificaciones de datos sin procesar); y el Aprendizaje Profundo (\textit{Deep Learning}). En partircular, el Deep Learning se caracteriza por ofrecer muy buenos resultados en detección de objetos en imágenes. El diagrama de Venn (Figura 1.2) muestra los subconjuntos de la Inteligencia Artificial.
\\

Por otra parte, la Visión Artificial (\textit{Computer Vision}) es la rama de la ciencia que recopila todos los métodos para adquirir, procesar, analizar y comprender las imágenes del mundo real para que puedan ser procesadas por un ordenador. En concreto, en este proyecto combinaremos la visión artificial para obtener imágenes de un robot en un entorno real simulado y el \textit{Deep Learning}, para el procesamiento e interpretación de esas imágenes y así dotar de cierta inteligencia necesaria al robot para que sea capaz de actuar en función de las condiciones de su entorno. 

\renewcommand{\figurename}{Figura}		
\begin{figure}[h]
	\centering
	 \includegraphics[scale=0.5]{images/diagrama-venn.jpg}
	 \caption{Diagrama de Venn}
\end{figure}

\newpage
\subsection{Redes Neuronales Convolucionales (CNNs)}

Existen distintas técnicas para llevar a la práctica el \textit{Deep Learning}. Una de ellas son las Redes Neuronales Convolucionales (CNN). Las redes neuronales son modelos de aprendizaje automático que intentan emular las neuronas de los sistemas nerviosos biológicos. Al igual que un cerebro humano, las redes neuronales pretenden establecer conclusiones de los datos que obtienen. En concretro, las Redes Neuronales Convolucionales utilizan filtros convolucionales (múltiples filtros o capas por los que va pasando la información) para realizar esta función.
\\

La arquitectura de las Redes Neuronales Convolucionales se basa en utilizar distintos niveles o capas en los que despúes de cada uno de ellos se añade una función para realiar un mapeo causal no-lineal. Es decir, cada capa está formada por dos subcapas: una capa convolucional (que realiza una operación de convolución), y una capa de submuestreo (\textit{pooling}, que genera características a partir de cálculos estadísticos del resultado de la convolución). Todas estas capas están conectadas de  tal forma que a la salida de todas ellas se puedan obtener conclusiones a partir de los resultados obtenidos. Cuantas más capas posea una red neuronal, más probabilidad hay de que los resultados obtenidos sean acertados. En la Figura 1.3 podemos encontrar un ejemplo de una red neuronal convolucional que clasifica un número manuscrito al número digital que corresponde. 

\renewcommand{\figurename}{Figura}		
\begin{figure}[h]
	\centering
	 \includegraphics[scale=0.45]{images/ejemplo_cnn.png}
	 \caption{Ejemplo de la arquitectura de una Red Neuronal Convolucional}
\end{figure}

Este trabajo se centra en la detección de objetos a través de imágenes, por lo que las Redes Neuronales Convolucionales son de gran utilidad ya que con ellas se puede procesar la imagen por secciones e identificar los objetos de manera más eficiente. Aplicando una red neuronal convolucional a nuestras imágenes podremos identificar el tamaño, la posición y el tipo de objeto que aparece en una imagen, siempre y cuando la clasificación del objeto identificado se encuentre dentro de las clases determinadas de la red neuronal. Para dicha implementación existen diversas plataformas tales como \texttt{Keras}, \texttt{TensorFlow}, \texttt{Caffe}, etc. En nuestro caso utilizaremos \texttt{TensorFLow} y, en concreto, TensorFlowjs, que es la librería para desarollarlo en JavaScript. El motivo de esta elección es que utilizaremos nuestra red en un entorno web, por lo que el hecho de que exista dicha librería en lenguaje JavaScript nos facilitará la incorporación de la red a nuestro entorno. Además, TensorFlow dispone de numerosos modelos pre-entrenados que podremos usar sin la necesidad de entrenar previamente un modelo y etiquetar infinidad de imágenes.

Algunos ejemplos en la vida real de la aplicación de Deep Learning y redes neuronales en procesamiento de imágenes son; Google Photos y su algoritmo para reconocer rostros y organizar las imágenes de la galería; reconocimiento inteligente de defectos en las piezas de una fábrica a través de una cámara que va captando las piezas que pasan a través de una cinta; la creación de ImageNet, una base de datos de imágenes clasificadas cuya creación se ayudó de técnicas de DeepLearning para clasificar las imágenes; Autopilot, la función de conducción autónoma de los coches Tesla que utiliza algoritmos de Deep Learning para materializar las abstracciones de su entorno y poder identificar los objetos que rodean al coche en todo momento y lograr así una conducción autónoma. 

\section{Robótica en educación}

En virtud del auge que está sufriendo la robótica en los últimos años, cada vez son más los profesionales demandados en este campo. Debido a esto, en paralelo con el auge de la robótica, también ha aumentado de manera exponencial la enseñanza de la róbotca en edades tempranas ya que apreder robótica puede ofrecernos una formación en diversos campos de forma simultánea: física, electrónica, informática...Además, el uso de aplicaciones de robótica refuerza la creatividad (a través del diseño de ideas y su desarrollo), el pensanmiento crítico (pensamientos lógicos) y prepara a los niños para un futuro en el que la robótica adquirirá un papel fundamental en la sociedad. Poco a poco se han ido introdución materias de robótica en la enseñanza. Por ejemplo, en la Comunidad de Madrid y en muchas otras se ha incluido en los proyectos formativos asignaturas de robótica. En contexto universitario se han creado grados específicos de robótica, como el grado de robótica en la Universidad de Alicante o el Grado en ingeniería robótica en la Universidad  Rey Juan Carlos. Además, en estos últimos años se han desarrollado numerosas plataformas orientadas a este fin. Entre ellas podemos encontrar OpenRoberta, iRobot y Kibotics (que es la que usaremos para este proyecto). A continuación se detallan cada una de ellas.

\subsection{OpenRoberta}

Es una plataforma \footnote{https://lab.open-roberta.org/} orientada a la programación por bloques en la que se pueden programar robots y otros sistemas hardware programabes como \texttt{Arduino}, \texttt{BBC micro} y \texttt{Calliope mini}. El objetivo de OpenRoberta es simplificar los conceptos de programación para poder introducir esta materia en los colegios. Ofrece un entorno de programación basado en la nube y desarrollado en código abierto que permite su uso en cualquier navegador sin necesidad de instalación. Dispone de varios robots y sistemas (mBot, Arduino,etc) con múltiples motores y sensores que pueden ser configurados a través de bloques. Ofrece múltiples idiomas y no es necesario registrarse para utilizarlo.

\renewcommand{\figurename}{Figura}		
\begin{figure}[h]
	\centering
	 \includegraphics[scale=0.34]{images/openRoberta.jpg}
	 \caption{Interfaz de OpenRoberta}
\end{figure}

\newpage
\subsection{iRobot Education}

iRobot es una empresa estadounidense que se dedica al diseño y fabricación de robots, principalmente orientados al uso en empresas, hogares e instituciones. Es conocida por el robot \textit{Roomba}, una aspiradora doméstica inteligente. Esta empresa ha creado iRobot Education \footnote{https://edu.irobot.com/}, una plataforma educativa destinada al aprendizaje de robótica. Esta empresa comercializa distintos robots, como por ejemplo el  robot móvil llamado \texttt{Create2}, muy parecido a su famoso aspirador Roomba, que los estudiantes y desarrolladores pueden programar para controlar los movimientos y el comportamiento del mismo de una manera sencilla a través de su plataforma. Otro robot que comercializan es el \texttt{iRobot Root rt0}, un robot con múltiples sensores (luz, movimiento...), con cámara, y con una superficie que permite introducir un lápiz de color para que el robot sea capaz de dibujar sobre una superficie en función del comportamiento programado. Esta plataforma es muy novedosa y  está destinado a aquellos estudiantes de secundaria y universidad, que empiezan a aprender robótica y a adquirir experiencia en programación, y sobre todo para familiarizarse tanto con el software como con el hardware.

\renewcommand{\figurename}{Figura}		
\begin{figure}[h]
\centering
  \begin{subfigure}[b]{0.24\textwidth}
  \centering
    \includegraphics[width=1.6\textwidth, height=\textwidth]{images/irobot.jpeg}
    \caption{Robot Create2}
    \label{fig:f1}
  \end{subfigure}\hspace{0.15\textwidth}
    \begin{subfigure}[b]{0.24\textwidth}
    \centering
    \includegraphics[width=1.6\textwidth, height=\textwidth]{images/irobot-r10.jpeg}
    \caption{Robot iRobot-r10}
    \label{fig:f2}
  	\end{subfigure}
  	\caption{Robots de iRobot Education}
\end{figure}

\newpage
\subsection{Kibotics} 

Es una plataforma \footnote{https://kibotics.org/} desarrollada por la Asociación de robótica e inteligencia artificial JdeRobot, destinada a la docencia \textit{STEM}. Este término hace referencia a \textit{Science, Technology, Engineering and  Mathematics} y se utiliza en términos de educación para apoyar la extensión del estudio de ingeniería, el inciio de la ingeniería, o en la escuela primaria. El objetivo de Kibotics es iniciar a niños y adolescentes en robótica y programación de robots. Se ejecuta en el navegador, por lo que no necesita instalación. Utiliza lenguajes intuitivos, como son Python y Scratch. Es especialmente interesante para los alumnos porque ofrece una gran variedad de robots programables tanto reales como simulados (PiBot, drones, Fórmula 1), y permite el desarrollo de prácticas avanzadas de visión artificial (robots con cámara). Esta es la plataforma que se va a utilizar para desarrollar este trabajo.

\renewcommand{\figurename}{Figura}
\begin{figure}[h]
\centering
	\begin{subfigure}[h]{\textwidth}
	\centering
	 \includegraphics[scale=0.15]{images/scratch+websim.png}
	 \caption{Scratch en Kibotics}
	\end{subfigure}
	\begin{subfigure}[h]{\textwidth}
	\centering
	 \includegraphics[scale=0.15]{images/python+websim.png}
	 \caption{Pyton en Kibotics}
	\end{subfigure}
	\caption{Interfaz de Kibotics con distintos lenguajes de programación}
\end{figure}

\chapter{Objetivos}
Una vez puesto en contexto el marco en el que se engloba este trabajo, en este capítulo se definen los objetivos generales y específicos planteados para la realización de este trabajo. A continuación se define la metodología llevada a cabo para la consecución de los objetivos. Finalmente se prensentan los requerimientos necesarios para evaluar el grado de logro de los objetivos.s 

\section{Objetivos}
El objetivo general de este trabajo es aumentar la funcionalidad de visión artifial de los robots de la plaraforma Kibotics. Actualmente los robots de dicha plataforma cuentan con una serie de funciones de visión que les permiten cierta interacción con su entorno. En concreto, las funciones disponibles son:

\begin{itemize}
\item \textit{\textbf{getImage}}: Devuelve una imagen capturada por la cámara del robot.
\item \textit{\textbf{getObjectColorRGB(lowval, highval)}}: Esta función filtra un objeto en la escena con un color dado utilizando OpenCVjs. Devuelve el centro (x,y) y el área del objeto detectado en la imagen
\item \textit{\textbf{getColoredObject(colorAsString)}}: Esta función filtra un objeto en la escena con un color, utilizando OpenCVhs y pasandole el color en formato \textit{string}. Devuelve el centro (x,y) y el área del objeto detectado en la imagen
\item \textit{\textbf{getColorCode(color)}}: Esta función devuelve valores binarios para el color dado, solamente si el color está en el conjunto de valores que el robot puede filtrar.
\end{itemize}

Estas funciones son muy útles, pero pueden llegar a ser poco intuitivas para un usuario principiate en el ámbito de la programación. Es por ello el objetivo de este trabajo es crear nuevas funciones que sean sencillas de usar para incorporarlas en el API de los robots de Kibotics. En concreto, los objetivos específicos de este trabajo se plantean en tres puntos:

\newpage
\begin{enumerate}
\item Mejorar el API de los robots de Kibotics utilizando OpenCVjs.
\item Mejorar el API de los robots de Kibotics materializando las redes neuronales y TensorFlowjs.
\item Validación experimental.
\end{enumerate}

Para ello, se predente crear un nuevo entorno en el que situar uno de los robots de Kibotics y desarollar los objetivos específicos mencionados anteriormente. En concreto se creará un entorno simulado que imite una pequeña ciudad con distintos elementos y que el robot utilizado represente un coche que circula por la ciudad. Concretamente, los objetivos específicos se prendenten materializar incorporando dos nuevas funciones de visión al API de los robots de Kibotics:

\begin{enumerate}
\item \textit{\textbf{dameCentroCarretera()}}: Función que devuelve la posición del robot respecto a la carretera por la que circula.
\item \textit{\textbf{ dameObjetoCiudad()}}: Función que devuelve un objeto identificado por el robot, la propabilidad que se ese objeto haya sido identificado de manera adecuada, y el área del mismo. En concreto estos objetos pueden ser señal de stop, persona, semáforo en rojo o semáforo en verde-amarillo
\end{enumerate}

\section{Metodología}
Para lograr los objetivos mencionados anteriormente, se han utilizado distintas herramientas y mecanimos que han permitido un desarollo óptimo del trabajo y un control y seguimiento del mismo. En primer lugar, se han establecido reuniones semanales en las que todos los miembros integrantes del equipo de trabajo, Jose María Cañas y Julio Vega, han participado de manera activa en la planificación de tareas, correciones de las tareas ya realizadas y establecicimiento de objetivos para la siguiente semana. 



\section{Requerimientos}

\chapter{Infraestructura}
\section{Kibotics}
Kibotics es una plataforma desarrollada por la Asociación de robótica e inteligencia artificial JdeRobot destinada a la educación en robótica. En la plataforma se pueden encontrar diversos ejercicios en los que el estudiante puede programar el comportamiento de un robot en diferentes situaciones; un coche de fórmula1 que sigue una línea en un circuito de carreras, un robot aspirador que se mueve por una habitación, un robot móvil que sigue una pelota... Para programar estos comportamientos el usuario puede elegir dos lenguajes; Python y Scratch. Por debajo de estos dos lenguajes, toda la lógica del entorno web está programada en Java Script. En este proyecto, se ha utilizado JavaScript para el desarrollo de nuevas funcionalidades para el API de los robots que se usan en Kibotics. 
\\

El funcionamiento interno de Kibotics está basado en un diseño formado por dos planos de actuación; un primer plano formado por los robots, sus sensores, y un API que permite el acceso a estos sensores; un segundo plano formado por un web worker en el que se incluye el código del usuario que programa el comportamiento del robot y un API de segundo plano que permite ordenar acciones al robot. La conexión entre el primer y el segundo plano se hace a través de un miniproxy que se encarga de enviar mensajes en ambos sentidos para el intercambio de información (instrucciones o parámetros de los sensores del robot). El motivo de este diseño es conseguir una división de tareas en dos planos de actuación para realizar aquellas necesarias en un primer plano (acceso a los sensores y actuadores del robot) y aquellas que puedan permitir una demora (procesamiento de las imágenes del robot, interpretación de los sensores como infrarrojos…) realizarlas en un segundo plano para que no bloqueen el flujo de ejecución del programa. En un primer prototipo, las nuevas funcionalidades del API de los robots están programadas en el API de primer plano, para comprobar su eficiencia y utilidad y en futuro programarlas en el API de segundo plano.

\renewcommand{\figurename}{Figura}		
\begin{figure}[t]
	\centering
	 \includegraphics[scale=0.4]{images/kibotics.png}
	 \caption{Flujo de datos de Kibotics entre el código de usuario y los sensores del robot.}
\end{figure}

\newpage
\section{JavaScript}
Java Script es un lenguaje de programación interpretado,  de alto nivel, orientado a objetos, débilmente tipado (permite flexibilidad en el manejo de variables) y que se apoya en otros lenguajes como Java o C. Es el lenguaje más utilizado para desarrollo web, ya que la mayoría de los navegadores tienen la capacidad de interpretar el código Java Script integrado en las páginas web. Este código se integra dentro de los ficheros HTML (aquellos que se encargan de gestionar todo lo que se muestra en pantalla) para realizar operaciones y definir la lógica de la página en el lado del cliente, ya que tiene acceso al DOM \footnote{https://developer.mozilla.org/es/docs/DOM} (Document Object Model),  permite la modificación de etiquetas HTML, envío de mensajes al navegador (alertas) y generación de gráficos en canvas\footnote{https://developer.mozilla.org/es/docs/Web/HTML/Canvas} (entre  muchas otras funciones). Existen numerosas API's (Aplication Programming Interface) en JavaScript del lado del cliente, que no son parte del lenguaje, pero que permiten utilizar numerosas funcionalidades y abstraer problemas complejos. Un ejemplo de ello es la API de Geolocalización, que permite obtener datos de ubicación a través de métodos de alto nivel (como por ejemplo getCurrentPosition) sin tener que preocupar al usuario de códigos de bajo nivel complejos para comunicarse con el hardware GPS del dispositivo. 
\\

Para este proyecto se ha utilizado Java Script para programar la inteligencia de la aplicación web que corre en el navegador y, en concreto, para programar el comportamiento del robot PiBot y que sea capaz de circular de manera autónoma por una carretera en un entorno simulado identificando los elementos de su entorno (señales de tráfico, semáforos y personas). Es decir, se ha utilizado JavaScript para programar nuevas funcionalidades de la API del robot Pibot. Para ello se han utilizado específicamente dos bibliotecas de Java Script: OpenCVjs y TensorFlowjs.

\subsection{Biblioteca OpenCVjs}
OpenCVjs \footnote{http://opencv.org} (Open Source Computer Vision Library) es una biblioteca de código abierto destinada a usarse en código Java Script y que incluye diversos algoritmos de visión artificial. Tiene una estructura modular, por lo que los algoritmos están organizados en paquetes o bibliotecas estáticas, entre los que podemos encontrar; core funcionality, image processing, video analysis… Esta librería resulta especialmente interesante en el trabajo con imágenes de visión artificial ya que nos permite realizar operaciones sobre las imágenes que pueden ser propias de la mente de una persona, como por ejemplo identificar un color, situar en una imagen un determinado color, detectar líneas y círculos, entre muchas otras. Además ofrece muchas otras funciones para procesar imágenes tales como detectar bordes, rotar o recortar imágenes, modificar el espacio de color, filtrar un color en la imagen, erosionar o suavizar los bordes de una imagen, etc.
\\

En este proyecto se ha utilizado la biblioteca de OpenCVjs para realizar filtros en las imágenes que capta el robot Pibot con el fin de identificar el color de la carretera y poder crear una función que retorne la posición en la que se encuentra el robot respecto a la carretera. Invocando a esta función, el usuario puede saber si el robot se situa en el centro de la carretera o se encuentra desviado.

\begin{figure}[h]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/imagen.JPG}  
  \caption{Imagen original}
  \label{fig:sub-first}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/gris.JPG}  
  \caption{Blanco y negro}
  \label{fig:sub-second}
\end{subfigure}


\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/blur.JPG}  
  \caption{Filtro gaussiano}
  \label{fig:sub-third}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/bordes.JPG}  
  \caption{Detección de bordes}
  \label{fig:sub-fourth}
\end{subfigure}
\caption{Ejemplos de procesamiento de imágenes con OpenCVjs}
\label{fig:fig}
\end{figure}

\newpage
\subsection{Biblioteca TensorFlowjs}
TensorFlowjs \footnote{https://www.tensorflow.org/} es una biblioteca de código abierto destina a realizar una computación numérica de alto rendimiento de una manera eficiente. Está desarrollado por Google Brain Team y ofrece numerosas herramientas para realizar tareas de aprendizaje automático (Machine Learing). A través de esta biblioteca se pueden construir y entrenar redes neuronales para posteriormente usarlas en clasificación y detección de objetos, patrones, correlaciones, o razonamientos similares a los de una mente humana. Además de construir y entrenar modelos, también se pueden ejecutar modelos existentes, así como volver a entrenar modelos existentes. 
\\

TensorFlowjs está diseñado para realizar operaciones en segundo plano, lo que hace que la utilización de sus funciones no bloquee en flujo de ejecución del hilo principal y se realicen operaciones de manera muy rápida en comparación con otras bibliotecas matemáticas de alto nivel (como por ejemplo NumPy). Además, está optimizado para un uso paralelo del hardware en la GPU, lo que hace que aumente el rendimiento al reducir el tiempo entre inferencias. Hoy en día esta biblioteca de código abierto se utiliza en ámbitos de investigación y en algunos productos de Google, como por ejemplo el reconocimiento de voz de Google, el traductor de Google que permite traducir imágenes en carteles en tiempo real, o el reconocimiento de imágenes en la galería de Google Fotos.

\section{A-Frame}
A-Frame [https://aframe.io/] es una plataforma de código abierto destinada a crear y simular experiencias 3D, de realidad virtual y de realidad aumentada. Se caracteriza por la gran facilidad de crear escenarios tridimensionales, simular entornos de la vida real y de añadir modelos sofisticados en diferentes formatos. Estas escenas son fácilmente configurables y modificables a través de ficheros, por lo que cambiar las luces de  una escena, los controles, los tamaños de los objetos, etc, es tan sencillo como cambiar una línea de código. Por ello A-Frame ofrece una gran versatilidad y utilidad, ya que toda una escena puede ser configurada en un archivo JavaScript e incluirse en un fichero HTML para su visualización web. Además, A-Frame es compatible con la mayoría de las bibliotecas (React (biblioteca de JavaScript), AngularJS, D3.js, Vue.js).  y marcos web existentes (Firefox, Google Chrome, Opera). 
\\

Algunos de los usos más comunes de A-Frame son el desarrollo de videojuegos, la recreación de entornos reales para realizar simulaciones y la  recreación de experiencias 3D ya que A-Frame tiene compatibilidad con Vive, Rift, Windows Mixed Reality, Daydream, GearVR y CardBoard por lo que se puede crear una escena en A-Frame y posteriormente verse inmerso en ella utilizando unas gafas de realidad virtual. Esta es la plataforma que usaremos en este proyecto para simular nuestro entorno; una mini-ciudad recreada con una alfombra infantil con carreteras, un robot móvil, y diferentes elementos de la ciudad tales como señales de tráfico, peatones y semáforos que van cambiando de color.

\renewcommand{\figurename}{Figura}		
\begin{figure}[h]
	\centering
	 \includegraphics[scale=0.25]{images/a-frame.png}
	 \caption{Interfaz de A-Frame}
\end{figure}

\newpage
\section{Blender}
Blender \footnote{https://www.blender.org/} es un software de código abierto destinado al diseño y animación 3D. A través de una interfaz gráfica se pueden crear objetos, personajes, escenas, animaciones y todo tipo de diseño tridimensional. Blender ofrece diversas características tales como el modelado (creación de objetos en un espacio tridimensional de manera digital), tracking (especificación del comportamiento y características de un objeto), animación (mediante secuencia de fotogramas) e iluminación (ajustar la luz deseada a la escena para cada objeto en cuanto a intensidad, color o posición). Es compatible con todas las versiones de Windows, macOS, GNU/Linux, Solaris, FreeBSD e IRIX. Para insertar los objetos creados en Blender en un entorno de A-Frame, se deben exportar dichos objetos en formato GL Transmssion Format (glTF) ya que este formato minimiza el tamaño de los ficheros permitiendo así ver con fluidez las animaciones creadas.
\\

En la práctica, algunos casos relevantes en los que se ha utilizado Blender han sido en la previsualización de escenas para películas como Spider-Man o Capitán América o la realización de largometrajes (largometraje Plumíferos \footnote{https://es.wikipedia.org/wiki/Plumíferos}) o cortometrajes (cortometraje Elephant Dream \footnote{https://orange.blender.org/}) utilizando Blender como herramienta principal. En este proyecto se ha utilizado Blender para el diseño de los objetos que conforman la ciudad recreada: señales de tráfico (señal de stop), semáforo animado que cambia de color y peatones animados que simulan la acción de caminar.

\renewcommand{\figurename}{Figura}		
\begin{figure}[h]
	\centering
	 \includegraphics[scale=1.5]{images/blender.jpg}
	 \caption{Interfaz de Blender}
\end{figure}

\chapter{Robot Autónomo en Ciudad Simulada}

En este capítulo se describe el proceso llevado a cabo para dotar al robot Pibot de la inteligencia necesaria para ser capaz de circular por una carretera en una ciudad simulada siguiendo el trazo de la misma e iteractuando con los obstáculos que se encuentre (semáforos, personas y señales de stop). En primer lugar se presenta el robot utilizado para el desarollo de este trabajo. En segundo lugar se explica el entorno creado para su desarrollo. Finalmente se describen dos tipos de comportamientos programados; \textit{\textbf{sigue carretera básico}}: el robot sigue el curso de la carretera; \textit{\textbf{sigue carretera avanzado}}: además de seguir la trayectoria de la carretera el robot es capaz de actuar en función de los obstáculos que se encuentre. 

\section{PiBot}
El robot utilizado para realizar este trabajo ha sido el robot Pibot, uno de los robots disponibles en la plataforma Kibotics para poder programar su comportamiento. Este robot se caracteriza por ser un robot móvil y por disponer de un hardware que le permite realizar cierta interacción con su entorno. En concreto, el robot PiBot está dotado con sensores infrarrojos y sensores infrasonidos para la detección de objetos próximos a él, y de una cámara frontal que le permite captar imágenes de su entorno a medida que el robot se mueve. El nombre PiBot hace referencia a que el todos los sensores y actuadores del robot están conectados a una Raspberry Pi para el control de los mismos (Pi-Bot). El motivo de la elección de este robot para realizar este trabajo es que gracias a la cámara que dispone se pueden realizar todas las tareas de visión artificial a través de las imágnes que capta. Estas imágenes serán procesadas, analizadas y permitirán modelar el comportamiento del robot en función de las mismas.

\renewcommand{\figurename}{Figura}		
\begin{figure}[!h]
\centering
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
    \includegraphics[width=0.75\textwidth, height=0.55\textwidth]{images/cap4/pibot-frontal.png}
    \label{fig:f1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
    \includegraphics[width=0.75\textwidth, height=0.55\textwidth]{images/cap4/pibot-lateral.png}
    \label{fig:f2}
  \end{subfigure}
  \caption{Robot PiBot}
\end{figure}

\newpage
\section{Ciudad Simulada}

Se ha recreado un entorno con el objetivo de introducir el robot PiBot dentro y que sea capa de  moverse y actuar. El propósito de este entorno es simular una ciudad interactiva pero conservando una estética infantil y educativa. Es por eso que la ciudad no es fotorealista y se ha buscado una apariencia más simple y vistosa, parecida a la de dibujos animados. Este entorno está compuesto por un conjunto de modelos y configuraciones (posición, tamaño, iluminación...) agrupados en un un fichero \textbf{.json}.

\subsection{Alfombra Infantil}
Se disponía de una alfombra real de dimensiones 2mx3m que ha sido digitalizada para usarse como base de la ciudad simulada. Esta base imita las carreteras de una ciudad con intersecciones, rotondas y líneas curvas. Se ha considerado que es un entorno óptimo ya que plantea distintos puntos en los que el robot tendrá que tomar decisiones, no es simplemente una carretera de un carril en línea recta.

\renewcommand{\figurename}{Figura}		
\begin{figure}[!h]
\centering
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
    \includegraphics[width=0.9\textwidth, height=0.7\textwidth]{images/cap4/alfombra-real.jpeg}
    \label{fig:f1}
    \caption{Alfombra real}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
    \includegraphics[width=0.9\textwidth, height=0.7\textwidth]{images/cap4/alfombra.jpg}
    \label{fig:f2}
    \caption{Alfombra digitalizada}
  \end{subfigure}
  \caption{Alfombra infantil}
\end{figure}


\newpage
\subsection{Señal de Stop}
Se ha modelado una señal de Stop en Blender a partir de las figuras primitivas geométricas básicas que ofrece dicho software (cubos, cilindros, conos...). Para insertar este modelo en el mundo basta con añadir el siguiente código al fichero de configuración del mismo:

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
{
 "tag": "a-gltf-model",
 "attr": {
	"position": { "x":-7, "y":-0.1, "z":11},
	"scale": { "x":-0.2, "y":0.2, "z":0.2},
	"rotation": {"x":0, "y":0},
	"src": "../assets/models/alfombra/StopSign.glb",
	"animation-mixer": "loop: repeat"
	}
}
\end{lstlisting}

\renewcommand{\figurename}{Figura}		
\begin{figure}[h]
	\centering
	 \includegraphics[scale=0.35]{images/cap4/stop.png}
	 \caption{Señal de Stop}
\end{figure}



\subsection{Semáforo animado}
Se ha modelado una señal de tráfico en Blender a partir de las figuras primitivas geométricas básicas que ofrece dicho software (cubos, cilindros, conos...). Además, se han añadido animaciones para que el color del semáforo vaya cambiando cada cierto intervalo de tiempo. Esto permitirá mayor interación del robot Pibot con dicho elemento, ya que además de identificar que hay un semáforo, tendrá que indentificar el color para actuar en consecuencia. Para insertar este modelo en el mundo basta con añadir el siguiente código al fichero de configuración del mismo:

\newpage
\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
{
 "tag": "a-gltf-model",
 "attr": {
	"position": { "x":-3.28, "y":-0.1, "z":0.60},
	"scale": { "x":-0.12, "y":0.12, "z":0.12},
	"rotation": {"x":0, "y":70},
	"src": "../assets/models/alfombra/semaforo_animado.glb",
	"animation-mixer": "loop: repeat"
	}
}
\end{lstlisting}

\renewcommand{\figurename}{Figura}		
\begin{figure}[h]
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth, height=1.2\textwidth]{images/cap4/rojo.png}
    \label{fig:f1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth, height=1.2\textwidth]{images/cap4/amarillo.png}
    \label{fig:f2}
  \end{subfigure}
  \hfill
    \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth, height=1.2\textwidth]{images/cap4/verde.png}
    \label{fig:f2}
  \end{subfigure}
  \caption{Semáforo simulado}
\end{figure}



\subsection{Peatones animados}
En la plataforma de Kibotics ya se disponía de modelos animados que simulaban peatones andando. Se han incorporado al fichero de la ciudad simulada mediante el siguiente código:

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
{
 "tag": "a-gltf-model",
 "attr": {
	"position": { "x":-3.28, "y":-0.1, "z":6},
	"scale": { "x":1, "y":1, "z":1},
	"rotation": {"x":0, "y":-90},
	"src": "../assets/models/alfombra/person2.glb",
	"animation-mixer": "loop: repeat",
	"animation": "property: position; from: -1.28 -0.1 6 ;
	to: -13.28 -0.1 6; dur: 40000; loop: true"
 }
}
\end{lstlisting} 

\renewcommand{\figurename}{Figura}		
\begin{figure}[!h]
\centering
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
    \includegraphics[width=0.4\textwidth, height=0.7\textwidth]{images/cap4/person.jpg}
    \label{fig:f1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
    \includegraphics[width=0.4\textwidth, height=0.7\textwidth]{images/cap4/person2.jpg}
    \label{fig:f2}
  \end{subfigure}
  \caption{Peatones animados}
\end{figure}

\hfill \break
\hfill \break
\hfill \break

Además, al fichero de connfiguración del mundo se ha añadido un cielo azul para iluminar toda la escena. En la \textbf{Figura 4.6} se muestran todos los elementos mencionados anteriormente integrados en la ciudad simulada.
\hfill \break
\hfill \break
\hfill \break
\renewcommand{\figurename}{Figura}		
\begin{figure}[h]
	\centering
	 \includegraphics[scale=0.4]{images/cap4/ciudad-simulada.png}
	 \caption{Elementos de la ciudad simulada}
\end{figure}

\newpage
\section{Sigue-Carretera Básico}
En este primer ejercicio se pretende dotar al robot PiBot de la inteligencia necesaria para que sea capaz de circular de manera autónoma por una careterra siguiendo el trazo de la misma, sin desviarse y a una velocidad constante (sin parones ni aceleraciones descompensadas). Para ello, este proceso se divide en dos pasos; \textbf{percepción}: el proceso mediante el cual el robot capta una imagen y es capaz de identificar la posición en la que se encuentra respecto a la carretera; \textbf{actuación}: en función de la posición en la que se encuentre, el robot se moverá de una manera o de otra.


\subsection{Percepción}

Para identificar la posición en la que se encuentra el robot PiBot respecto a la carretera primero hay que identificar la carretera. Para ello se ha utilizado OpenCVjs como herramienta para utilizar filtros de color y poder transformar la imagen de la cámara del robot en una imagen binaria en la que los píxeles blancos representen la carretera y los píxeles negros el resto de información de la imagen que no interesa. A continuación se describe el proceso realizado:
\\

1.- Captura de la imagen de la cámara del robot PiBot.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
var imagen = myRobot.dameImagen();
\end{lstlisting}

2.- Filtrar el color gris en la imagen capturada.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
let lower = [50, 40, 40, 0];
let higher = [100, 90, 255, 255];
let src = cv.imread(imagen);
let dst = new cv.Mat();
let low = new cv.Mat(src.rows, src.cols, src.type(), lower);
let high = new cv.Mat(src.rows, src.cols, src.type(), higher);
cv.inRange(src, low, high, dst);
\end{lstlisting}

3.- Realizar transformaciones morfológicas: erosión y dilatación. Este paso se realiza para evitar falsos positivos. Es decir, evitar que pequeños puntos de la imagen que tienen el mismo color que la carretera sean identificados como parte de la carretera.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
let M = cv.Mat.ones(5, 5, cv.CV_8U);
let anchor = new cv.Point(-1, -1);
cv.dilate(dst, dst, M, anchor, 1, cv.BORDER_CONSTANT,
		 cv.morphologyDefaultBorderValue());
cv.erode(dst, dst, M, anchor, 3, cv.BORDER_CONSTANT,
		 cv.morphologyDefaultBorderValue());
\end{lstlisting}

4.- Finalmete se aplica un suavizado a la imagen para eliminar el ruido de la misma.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
cv.medianBlur(dst, dst, 5);
\end{lstlisting}

\begin{figure}[h]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth, height=.65\linewidth]{images/cap4/camara-pibot.png}  
  \caption{Imagen del PiBot}
  \label{fig:sub-first}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/filtro-color.png}  
  \caption{Filtro de color}
  \label{fig:sub-second}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/erosion-dilatacion.png}  
  \caption{Erosión y dilatación}
  \label{fig:sub-third}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/erode.png}  
  \caption{Suavizado}
  \label{fig:sub-fourth}
\end{subfigure}
\caption{Proceso de filtrado del color de la carretera de una imagen del robot Pibot}
\label{fig:fig}
\end{figure}

Llegados a este punto, se dispone de una imagen binaria en la que los píxeles blancos representan la carretera y los píxeles negros el resto de información que se pretende evitar. Finalmente, para calcular el centro de la carretera, se aplica la función de OpenCVjs \textit{cv.moments} que nos aproxima el centro de un triángulo que encierra todos los píxeles blancos de la imagen. De este modo podemos obetener de una forma bastante aproximada el centro de la carretera.

\newpage
\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
let src2 = dst;
let dst2 = cv.Mat.zeros(src2.rows, src2.cols, cv.CV_8UC3);
let contours = new cv.MatVector();
let hierarchy = new cv.Mat();
cv.findContours(src2, contours, hierarchy, cv.RETR_CCOMP,
				 cv.CHAIN_APPROX_SIMPLE);
let cnt = contours.get(0);

let Moments = 0;
if (typeof cnt !== 'undefined') {
	Moments = cv.moments(cnt, false);
}else{
	Moments = 0;
}

\end{lstlisting}

Todo este proceso se engobla en una funcion nueva creada \textit{\textbf{myRobot.dameCentroCarretera()}} que pasa a formar parte del API del robot PiBot.

\subsection{Actuación}

En primer lugar, se arranca el robot y se define un bucle infinito que permitirá ir captando y procesando imágenes cada cierto intervalo de tiempo. En cada iteración se invoca a la función \textit{\textbf{dameCentroCarretera()}}, que devuelve la posición del centro de la carretera detectada por el robot PiBot. Con este valor se invoca a la función \textit{\textbf{maniobrar()}}, que será la encargada de corregir la trayectoria del robot.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
myRobot.move(speed,0,0);                   
while (true){
	var posicion = await myRobot.dameCentroCarretera();
	console.log(posicion);
	await maniobrar(posicion);

	await myRobot.sleep(0.15);
}
\end{lstlisting}

A continuación, la función \textit{\textbf{maniobrar}} toma como parámetro de entrada la posición respecto a la carretera en la que se encuentra el robot y, en función de unos intervalos definidos, realizará un giro de una manera o de otra.

\newpage
\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
async function maniobrar(posicion){
    if (posicion>(centro-margen) 
    	& posicion<(centro+margen)){
        console.log('RECTO');
        myRobot.move(speed,0,0);

    }else if (posicion>(centro+margen) 
    		& posicion<(centro+margen+rango1)){
        console.log('Giro DERECHA');
        myRobot.move((speed-0.1),-giro1,0);

    }else if (posicion>(centro+margen+rango1) 
    		& posicion<(centro+margen+rango1+rango2)){
        console.log('Giro GRANDE DERECHA');
        myRobot.move((speed-0.2),-giro2,0);


    }else if (posicion<(centro-margen) 
    		& posicion>(centro-margen-rango1)){
        console.log('Giro IZQUIERDA');
        myRobot.move((speed-0.1),giro1,0);

    }else if (posicion<(centro-margen-rango1) 
    	& posicion>(centro-margen-rango1-rango2)){
        console.log('Giro GRANDE IZQUIERDA');
        myRobot.move((speed-0.2),giro2,0);

    }else{
       myRobot.move(speed,0,0)
    }
}
\end{lstlisting}

Con este manejador el robot es capaz de circular de manera autónoma siguiendo la carretera y corrijiendo su trayectoria en función de los giros y curvas de la carretera por la que circula. A través de la experimentación se han definido unos parámetros para el funcionamiento óptimo de dicho algoritmo:

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
		const speed = 0.75;
		const centro = 40;
		const margen = 5;
		const rango1 = 10;
		const rango2 = 10;
		const giro1 = 0.002;
		const giro2 = 0.003;
\end{lstlisting}
 

\subsection{Validación Experimental}
El mecanismo de actuación mencionado anteriormente se ha validado de forma experimental y verificado su funcionamiento. A continuación de muestra un ejemplo de una iteración en la que el robot Pibot realiza un recorrido por la ciudad simulada.

\begin{figure}[h]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/1.png}  
  \caption{Punto de partida}
  \label{fig:sub-first}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/2.png}  
  \caption{Giro curva hacia la izquierda}
  \label{fig:sub-second}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/3.png}  
  \caption{Giro curva hacia la derecha}
  \label{fig:sub-third}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/4.png}  
  \caption{Continúa recto}
  \label{fig:sub-fourth}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/5.png}  
  \caption{Trazado de la curva de la rotonda}
  \label{fig:sub-first}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/6.png}  
  \caption{Trazado de la curva de la rotonda}
  \label{fig:sub-second}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/7.png}  
  \caption{Toma la primera salida de la rotonda}
  \label{fig:sub-third}
\end{subfigure}
\caption{Validación experimental Sigue-Carretera Básico}
\label{fig:fig}
\end{figure}

Eta iteración puede visualizarse en este enlace: https://youtu.be/iQwMiKAp8dQ. Otra iteración partiendo desde otro punto puede visualizarse en este otro enlace: https://youtu.be/ChbS4QYtZiA.

\newpage
\section{Sigue-Carretera Avanzado}
En este segundo ejercicio se pretende dotar al robot PiBot de la inteligencia necesaria para que sea capaz de circular de manera autónoma por una careterra siguiendo el trazo de la misma, sin desviarse y a una velocidad constante (sin parones ni aceleraciones descompensadas) y además que sea capaz de identificar los elementos de su entorno y actuar en consecuencia. En concreto el robot PiBot será capaz de identificar señales de Stop y detenerse, semáforos y su estado correspondiente (rojo, amarillo y verde) para continuar su trayectoria únicamente cuando el semáforo esté en verde, y peatones que estén en su trayectoria de tal modo que el robot PiBot continúe circulando únicamente cuando el peatón haya desaparecido de su rango de visión. Para lograr este propósito se dividirá el proceso en dos pasos; \textbf{percepción}: el proceso mediante el cual el robot capta las imágenes y es capaz de identificar que hay uno de los elementos mencionados anteriormente en ella; \textbf{actuación}: en función del elemento que haya detectado actuará en consecuencia.


\subsection{Percepción}
Para lograr que el robot PiBot sea capaz de identificar los elementos que hay en las imágenes que capta utilizaremos las redes neuronales mencionadas en el \textbf{apartado 1.2.1}. En concreto, la biblioteca TensorFlowjs (\textbf{apartado 3.2.1}) nos ofrece una serie de herramientas ya construidas para trabajar con redes neuronales. Dicha biblioteca posee una serie de modelos de redes neuronales ya entrenados que utilizaremos en este trabajo para lograr los objetivos. En primer lugar, como toma de contacto para conocer el funcionamiento y la API de TensorFlowjs, se ha desarrollado una aplicación web utilizando el modelo MNIST, un modelo preentrenado para la identificación de números manuscritos. Una vez familiarizado con estas herramientas se ha procedido a utilizar el modelo de red neuronal ya entrenado COCO-SSD, que permite identificar objetos comunes en imágenes. 


\subsubsection{MNIST}
MNIST es una base de datos de números manuscritos que estan etiquetados con su correspondiente número digital. El \textit{dataset} (cojunto de todas las muestras) está formado por más de 60.000 imágenes y 10.000 imágenes que se han usado como imágenes de prueba para validar que la clasificación se hace de manera adecuada. Este dataset puede ser incorporado a la biblioteca de TensorFlowjs para construir una red neuronal y crear las herramientas necesarias para la clasificiación de números manuscritos en tiempo real.
\\

Se ha construido una aplicación web para la clasificación de números manuscritos a partir de imágenes en vivo. El funcionamiento de la aplicación consiste en la captación cotinua de imágnes a través de la cámara web. A estas imágenes se les aplica un filtro de color para transformarlas en una imagen binaria de tal modo que los píxeles blancos representen las partes  del dígito que queremos clasificar y los píxeles negros sean el resto de información de la imagen que no intersa. A través del modelo de red neuronal previamente mencionado, las imágenes que capta el PiBot son contrastadas con las 60.000 imágenes del dataset de tal modo que la aplicación web es capaz de proporcionar un porcentaje de semejanza con cada uno de los dígitos. El mayor porcentaje de todos será el que la aplicación asigne como dígito identificado, de tal modo que se iluminará dicho dígito en un display que contiene los dítigidos del 0 al 9.

\renewcommand{\figurename}{Figura}		
\begin{figure}[h]
	\centering
	 \includegraphics[scale=0.18]{images/cap4/mnist.png}
	 \caption{Aplicación web MNIST}
\end{figure}

Una demosración del funcionamiento en vivo de esta aplicación puede encontrarse en el siguiente enlace: https://youtu.be/7eZKIZAyolo

\subsubsection{COCO-SSD}
COCO-SSD (Common Ibjects in Context - Single Shot MultiBox Detection) es un dataset formado por más de 300.000 imágenes en las que se etiquetan más de 80 categorías de objetos que se encuentran en ellas. Entre estas categorías podemos encontrar perros, gatos, teléfono móvil, skateboard, bicicleta... y lo más importante de todo es que entre estas categorías se encuentran personas, señal de stop y semáforo. A partir de este dataset se puede construir una red neuronal en TensorFlowjs para la detección de objetos. Para familiarizarnos con el uso de esta red neuronal se ha creado una aplicación web que capta imágenes en tiempo real de la cámara web y las procesa para mostrar en la imágen bounding-boxes (recuadros) que identifican los objetos reconocidos.

\renewcommand{\figurename}{Figura}		
\begin{figure}[h]
	\centering
	 \includegraphics[scale=0.45]{images/cap4/coco.png}
	 \caption{Aplicación web deteción de objetos COCO-SSD}
\end{figure}

Una demosración del funcionamiento en vivo de esta aplicación puede encontrarse en el siguiente enlace: https://youtu.be/APm9VxMxFg4



\subsubsection{COCO-SSD en PiBot}
Una vez probado el funcionamiento de esta red neuronal, se ha procedido a integrarla en el entorno de trabajo de websim para que el robot PiBot sea capaz de utilizar dicha red neuronal en su API para detectar en tiempo real los objetos que aparecen en sus imágnes. Para ello se han realizado los siguientes procesos:
\\

1.- Cargar la biblioteca TensorFlowjs y el modelo de red neuronal COCO-SSD en el entorno web principal de websim.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   framexleftmargin = 1em,
                   basicstyle=\small]
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.0.0/
                                        dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/
                                              coco-ssd"></script>
\end{lstlisting}

Captura de la imagen de la cámara del robot PiBot.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
var imagen = myRobot.dameImagen();
\end{lstlisting}

2.- Predicción de los objetos que se ecuentran en la imagen. La función model.detect es una función de TensorFlowjs que nos devuelve un array formado por todos los datos en función de las prediciones de los objetos que se encuentran en la imagen que se analiza.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
var predictions = await model.detect(img);
\end{lstlisting}

3.- Construcción el objeto que se devuelve con toda la información de la predicción. El objeto está formado por tres parámetros; \textbf{clase}: el tipo de objeto identificado (semáforo, señal de stop o persona); \textbf{probabilidad}: la probabilidad con la que ese objeto identificado haya sido correctamente identificado; \textbf{area}: el área que ocupa ese objeto dentro de la imagen. La probabilidad nos servirá para establecer un umbral que consideraremos como objeto detectado correctamente. El área se utilizará para identificar la distancia a la que se encuentra el objeto identificado del robot PiBot.


\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
var objetoCiudad = {clase: null, probabilidad: null, area:null};
if (predictions.length != 0){
	objetoCiudad.probabilidad = predictions[0].score;
	objetoCiudad.clase = predictions[0].class;
	var bbox = predictions[0].bbox;
	objetoCiudad.area = (bbox[0]+bbox[3]);
}
\end{lstlisting}

De este modo podemos identificar los objetos que aparecen en las imágenes que capta el robot PiBot.


\renewcommand{\figurename}{Figura}		
\begin{figure}[!h]
\centering
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
    \includegraphics[scale=0.342]{images/cap4/sin-deteccion.jpg}
    \label{fig:f1}
    \caption{Imagen capturada por el robot PiBot}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
    \includegraphics[scale=0.4]{images/cap4/con-detection.jpg}
    \label{fig:f2}
    \caption{Detección de objetos}
  \end{subfigure}
  \caption{Detección de objetos  en imagen capturada por el robot PiBot}
\end{figure}

\newpage
En el caso de que se detecte un semáforo, hay que realizar un paso más: identificar el color del semáforo. Para ello se han utilizado las funciones de OpenCVjs y los filtros de color que nos proporciona esta libreria (del mismo modo que se filtra el color de la carretera en el Sigue-Carretera Básico).
\\

Detectamos cuando se identifica un semáforo.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
if(objetoCiudad.clase == "traffic light"){
	var color = colorSemaforo(img);
	objetoCiudad.clase = "traffic light " + color;
}\end{lstlisting}

Aplicamos un filtro de color para detectar únicamente el color rojo.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
let src = cv.imread(img);
/* ROJO */
let lower = [100, 0, 0, 0];
let higher = [255, 0, 10, 255];

let dst = new cv.Mat();
let low = new cv.Mat(src.rows, src.cols, src.type(), lower);
let high = new cv.Mat(src.rows, src.cols, src.type(), higher);
cv.inRange(src, low, high, dst);
}\end{lstlisting}

Llegados a este punto disponemos de una imagen binaria. Los pixeles blancos representan el color filtrado en la imagen (rojo) y los pixeles negros el resto de información que no interesa.

\renewcommand{\figurename}{Figura}		
\begin{figure}[!h]
\centering
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
    \includegraphics[scale=0.25]{images/cap4/rojo2.png}
    \label{fig:f1}
    \caption{Imagen capturada por el robot PiBot}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
  \centering
    \includegraphics[scale=0.5]{images/cap4/rojo-filtrado.png}
    \label{fig:f2}
    \caption{Filtro del color rojo en la imagen}
  \end{subfigure}
  \caption{Filtro del color del semáforo}
\end{figure}

Para identificar si el semáforo está en rojo o no, sobre esa imagen binaria aplicamos la función de de OpenCVjs \textit{countNonZero}, que nos devuelve el número de píxeles de una imagen que no son negros. De este modo podemos establecer un umbral para identificar cuando el semáforo está en rojo y cuando no.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
var area = cv.countNonZero(dst); // Me devuelve los pixeles blancos

if(area>120){
	var color = "rojo";
}else{
	var color = "verde-amarillo";
}
\end{lstlisting}

Todo este proceso se engobla en una funcion nueva creada \textit{\textbf{myRobot.dameObjetoCiudad()}} que pasa a formar parte del API del robot PiBot.

\subsection{Actuación}
Una vez se dispone de una nueva función en el API del robot que nos devuelve los objetos que detecta en la imagen, se ha procedido a programar un comportamiento del robot en función de los objetos que detecte. Al detectar una señal de Stop, el robot se detendrá durante unos segundo hasta que volverá a retomar la marcha. Si detecta una persona, el robot se detendrá hasta que la persona quede fuera su campo de visión. Si el robot detecta un semáforo con el estado \textit{"verde-amarillo"} no detendrá su marcha. Por último, si el robot detecta un semáforo con el estado \textit{"rojo"} detendrá su marcha hasta que vuelva a detectar un semárofor \textit{"verde-amarillo"}. A continuación se describe el proceso llevado a cabo para programar dicho comportamiento:

\newpage
En primer lugar, se arranca el robot y se define un bucle infinito que permitirá ir captando y procesando imágenes cada cierto intervalo de tiempo. En cada iteración se invoca a la función \textit{\textbf{dameObjetoCiudad()}}, que devuelve los objetos detectados por el robot PiBot. Con esta información se invoca a la función \textit{\textbf{actuar()}}, que será la encargada de actuar en función del objeto detectado.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
myRobot.move(speed,0,0);                   
while (true){
	var objetoCiudad = await myRobot.dameObjetoCiudad();
	console.log(objetoCiudad);
	await actuar(objetoCiudad);

	await myRobot.sleep(0.15);
}
\end{lstlisting}

La función \textit{\textbf{actuar}} se compone de una sentencia condicional \textit{if else} que irá valorando los objetos que se hayan detectado y en función de ello actuar de una manera o de otra.
\\

En primer lugar, si detecta un semáforo en rojo con cierta probabilidad y que esté considerablemente cerca (área superior a un umbral), el robot se detiene hasta que nuevamente vuelva a detectar el semáforo en verde.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
if((objetoCiudad.clase == "traffic light rojo") &
	(objetoCiudad.probabilidad>0.6) &
	(objetoCiudad.area>200)){
	
	myRobot.move(0,0,0);

	do{
	 var objetoCiudad = await myRobot.dameObjetoCiudad();
	 console.log(objetoCiudad);
	 await myRobot.sleep(0.15);

	}while(objetoCiudad.clase != "traffic light verde-amarillo")

	myRobot.move(speed,0,0);
}
\end{lstlisting}

En segundo lugar, si detecta una señal de stop con cierta probabilidad y que esté considerablemente cerca (área superior a un umbral), el robot se detiene durante unos segundos. El robot bloqueará la detección durante unos segundo adicionales para asegurar que avanza y sobrepasa la señal de Stop y no vuelve a detenerse al volver a detectarla.

\newpage
\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
}else if((objetoCiudad.clase == "stop sign") &
        (objetoCiudad.area>80) & 
        (objetoCiudad.probabilidad>0.6){
 console.log('STOP!');
 myRobot.move(0,0,0);
 await myRobot.sleep(3);
 
 /* Reanudar la marcha */
 myRobot.move(speed,0,0);
 
 /* Sobrepasar el Stop */
 await myRobot.sleep(3);
}
\end{lstlisting}

En tercer lugar, si detecta un persona con cierta probabilidad y que esté considerablemente cerca (área superior a un umbral), el robot se detiene hasta que deje de detectar a una persona.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
if((objetoCiudad.clase == "person") &
	(objetoCiudad.probabilidad>0.6) &
	(objetoCiudad.area>150)){
	
	myRobot.move(0,0,0);

	do{
	 var objetoCiudad = await myRobot.dameObjetoCiudad();
	 console.log(objetoCiudad);
	 await myRobot.sleep(0.15);

	}while(objetoCiudad.clase != "person")

	myRobot.move(speed,0,0);
}
\end{lstlisting}

Finalmente, si el robot detecta un objeto distinto a todos estos, procederá a actuar como el Sigue-Carretera Básico. Tomará una imagen, calculará el centro de la carretera y realizará una maniobra u otra en función de la posición en la que se encuentre.

\begin{lstlisting}[backgroundcolor = \color{light-gray},
				   aboveskip = 2em,
				   belowskip = 2em,
                   xleftmargin = 2cm,
                   framexleftmargin = 1em,
                   basicstyle=\small]
}else{
	var posicion = await myRobot.dameCentroCarretera();
	console.log(posicion);
	await maniobrar(posicion);
}
\end{lstlisting}


\subsection{Validación Experimental}
El mecanismo de actuación mencionado anteriormente se ha validado de forma experimental y verificado su funcionamiento. A continuación de muestra un ejemplo de una iteración en la que el robot Pibot realiza un recorrido por la ciudad simulada.

\begin{figure}[h]
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/1-1.png}  
  \caption{Punto de partida}
  \label{fig:sub-first}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/2-1.png}  
  \caption{Se detiene al detectar un Stop}
  \label{fig:sub-second}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/3-1.png}  
  \caption{Se detiene al detectar una persona}
  \label{fig:sub-third}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/4-1.png}  
  \caption{La persona sale del campo de visión del robot}
  \label{fig:sub-fourth}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/5-1.png}  
  \caption{Continúa avanzando}
  \label{fig:sub-first}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/6-1.png}  
  \caption{Se detiene al detectar un semáforo en rojo}
  \label{fig:sub-second}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.7\linewidth]{images/cap4/7-1.png}  
  \caption{Continúa avanzando al detectar un semáforo en verde}
  \label{fig:sub-third}
\end{subfigure}
\caption{Validación experimental Sigue-Carretera Avanzado}
\label{fig:fig}
\end{figure}

Eta iteración puede visualizarse en este enlace: https://youtu.be/AjIv9zFCsWI

\chapter{Conclusiones}
REvisitar los objetivos y ver cómo se han satisfecho todos.
\section{Conclusiones}
\section{Líneas futuras}
Probar estas funcionalidades con robots reales.
Entrenar redes neuronales para otros objetos.
Probarlo con robots con mayor DINAMISMO. OPtimizar computacionalmente. Un coche que siga a otro coche, un dron que siga a una per

\chapter{Bibliografía}

\end{document}
